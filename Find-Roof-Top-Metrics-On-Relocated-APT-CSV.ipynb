{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2377f775-f8da-4707-99aa-7978120f41d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import SedonaContext\n",
    "from sedona.spark.utils import KryoSerializer, SedonaKryoRegistrator\n",
    "from sedona.spark import SedonaContext \n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"roof-top-metrics\")\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    # .config(\"spark.executor.memory\", \"8g\") \n",
    "    # .config(\"spark.executor.cores\", \"8g\") \n",
    "    # .config(\"spark.executor.instances\", \"4\") \n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \n",
    "    .config(\"spark.network.timeout\", \"120s\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "spark = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d86fdf1-1c82-4b26-b586-6eb98c40f1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Laod Search logs from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfc69d2-9a45-4d9f-8292-9d48c47c7d65",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769016380666}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "search_logs = spark.read.csv(\"/mnt/opas/opas-source/apt-roof-top-accuracy-improvement/rooftop_prod_logs_run_id_20251209_00032117.csv\", header=True, inferSchema=True, sep=\",\", quote='\"', escape='\"')\n",
    "\n",
    "print(f\"Total Search Count: {search_logs.count()}\")\n",
    "usa_search_logs = search_logs.filter(\"country == 'USA'\")\n",
    "\n",
    "usa_total_count = usa_search_logs.count()\n",
    "usa_rooftop_count = usa_search_logs.filter(\"rooftop_match == 1\").count()\n",
    "usa_outside_rooftop_count = usa_search_logs.filter(\"rooftop_match ==0\").count()\n",
    "\n",
    "# Calculate percentages\n",
    "rooftop_percentage = (usa_rooftop_count / usa_total_count) * 100\n",
    "outside_rooftop_percentage = (usa_outside_rooftop_count / usa_total_count) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"USA Search Count: {usa_total_count} ({100:.2f}%)\")\n",
    "print(f\"USA Search On RoofTop Count: {usa_rooftop_count} ({rooftop_percentage:.2f}%)\")\n",
    "print(f\"USA Search Outside RoofTop Count: {usa_outside_rooftop_count} ({outside_rooftop_percentage:.2f}%)\")\n",
    "\n",
    "search_logs.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ea90a6-6551-40aa-8942-edede7018590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Country Specific search logs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42643f9-a569-4a72-a038-e3b46d79ae7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming you have a Spark DataFrame named search_logs\n",
    "# Group by country and rooftop_match to get the count\n",
    "grouped_result = search_logs.groupBy(\"country\", \"rooftop_match\").count()\n",
    "\n",
    "# Calculate total count per country\n",
    "total_count_per_country = grouped_result.groupBy(\"country\").agg(F.sum(\"count\").alias(\"total_count\"))\n",
    "\n",
    "# Join the two DataFrames to calculate the percentage\n",
    "result_with_percentage = grouped_result.join(total_count_per_country, \"country\") \\\n",
    "    .withColumn(\"percentage\", F.format_number((F.col(\"count\") / F.col(\"total_count\") * 100), 2)) \\\n",
    "    .orderBy(F.col(\"country\").desc())\n",
    "\n",
    "# Show the result\n",
    "display(result_with_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0660ecb-6068-45e0-acb0-f0fb5d6b6c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load APT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7a595437-44f1-45e1-b977-ae24c6778524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "apt_dataset = spark.table(\"preprocess_prod.layer_14533\")\n",
    "apt_dataset = apt_dataset.filter(expr(\"exists(tags, x -> x.tagKey.key = 'metadata:country' AND x.value = 'USA')\"))\n",
    "\n",
    "display(apt_dataset)\n",
    "# DBTITLE 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b6b1f7-0c1e-4cf8-84fa-7244c79344d1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768987635133}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def convert_to_unsigned(layer_id, high, low):\n",
    "    # layer_id = 14533\n",
    "    unsigned_high = int(high) & ((1 << 64) - 1)\n",
    "    unsigned_low = int(low) & ((1 << 64) - 1)\n",
    "    return f\"{layer_id}_{unsigned_high}_{unsigned_low}\"\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "apt_Data_with_unsigned_id = apt_dataset.withColumn(\"unsigned_id\", F.udf(convert_to_unsigned, \"string\")(\"id.layerId\",\"id.high\", \"id.low\")).withColumn(\"location\", F.expr(\"CONCAT('POINT(', lat, ' ', lng, ')')\"))\n",
    "display(apt_Data_with_unsigned_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e0609e9-3d01-44ae-a836-7b93a0f59efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Find missing search log APT Ids in layer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc371fe-2094-4c6c-a59f-6104ca09c314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "missing_ids_from_search_log = search_logs.alias(\"search\").join(apt_Data_with_unsigned_id.alias(\"layer\"), col(\"search.provider_uid\") == col(\"layer.unsigned_id\"), \"leftanti\")\n",
    "missing_ids_from_search_log.display()\n",
    "print(missing_ids_from_search_log.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ac5bc5-e7a4-4578-8a65-fcc20cac87eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load APT Relocated CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9fae29-6620-488e-aa84-f1fff60f156f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relocated_apt_ds = spark.read.csv(\"/mnt/opas/opas-source/apt-roof-top-accuracy-improvement/relocated_usa_apt.csv\", header=False, inferSchema=True, sep=\"|\", quote='\"', escape='\"').withColumnRenamed(\"_c0\", \"singed_id\") \\\n",
    ".withColumnRenamed(\"_c1\", \"location\") \\\n",
    ".withColumnRenamed(\"_c2\", \"relocated_location\") \\\n",
    ".withColumnRenamed(\"_c3\", \"relocated_bfp\") \\\n",
    ".withColumnRenamed(\"_c4\", \"uuid\") \\\n",
    ".withColumnRenamed(\"_c5\", \"comment\") \\\n",
    ".withColumnRenamed(\"_c6\", \"hn_number\") \\\n",
    ".withColumnRenamed(\"_c7\", \"street_name\") \\\n",
    ".withColumnRenamed(\"_c8\", \"apt_id\") \n",
    "\n",
    "display(relocated_apt_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b9d58b-6636-435c-a1e1-dd4e18c99411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join search result apt with relocated apt, add final location column, at last find does this APT with final location inside BFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a3f925-ab55-4411-a98d-9b067699c867",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769435741660}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_logs_with_relocated_apt = usa_search_logs.alias(\"search\").join(relocated_apt_ds.alias(\"relocated_apt\"), col(\"search.provider_uid\") == col(\"relocated_apt.apt_id\"), \"left\")\n",
    "\n",
    "display(search_logs_with_relocated_apt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e5e2ed-bf0c-46d7-bef2-00169fc42e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load BFP from layer 53801 OSM Visualization of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9f46f6-e955-42f6-b9ce-2ad5ba3f3bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "viz_dataset = spark.table(\"preprocess_dev.layer_53801\")\n",
    "\n",
    "bfp_dataset = viz_dataset.filter(col(\"wkt\").isNotNull()) \\\n",
    "                         .filter(expr(\"exists(tags, x -> x.tagKey.key = 'building' AND x.value = 'yes')\"))\n",
    "\n",
    "print(f\"viz_dataset Count: {viz_dataset.count()}\")\n",
    "print(f\"bfp_dataset Count: {bfp_dataset.count()}\")\n",
    "display(bfp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cadc16d-cca9-454c-bdcf-53602918fcc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "relocated_apt_ds_with_bfp_data = relocated_apt_ds.alias(\"relocated_apt\") \\\n",
    ".join(bfp_dataset.alias(\"bfp\"), expr(\"ST_Intersects(ST_GeomFromText(bfp.wkt), ST_GeomFromText(relocated_apt.relocated_location))\"), \"left\") \\\n",
    ".withColumn(\"apt_inside_bfp\", when(col(\"bfp.wkt\").isNull(), False).otherwise(True))\n",
    "\n",
    "display(relocated_apt_ds_with_bfp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515a7e74-a854-478f-81c7-2019ece18108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "num_partitions = 1000\n",
    "relocated_apt_ds = relocated_apt_ds.repartition(num_partitions)\n",
    "bfp_dataset = bfp_dataset.repartition(num_partitions)\n",
    "\n",
    "relocated_apt_ds_with_bfp_data = (\n",
    "    relocated_apt_ds.alias(\"relocated_apt\")\n",
    "    .join(bfp_dataset.alias(\"bfp\").hint(\"shuffle\"), \n",
    "          expr(\"ST_Intersects(ST_GeomFromText(bfp.wkt), ST_GeomFromText(relocated_apt.relocated_location))\"), \n",
    "          \"left\")\n",
    "    .withColumn(\"apt_inside_bfp\", when(col(\"bfp.wkt\").isNull(), False).otherwise(True))\n",
    ")\n",
    "\n",
    "display(relocated_apt_ds_with_bfp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b3340ef-06ac-4944-8156-4bd2c480bb2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load BFP from MCR\n",
    "MCR data already loaded from LoadDataFromMcr.scala(orbis-addressing-bulk-apt-tools) at delta table: preprocess_prod.bfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65955ff5-dd43-4e13-b046-82109288e8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mcr_usa_bfp_dataset = spark.table(\"preprocess_prod.bfp\").filter(col(\"licenseZone\") == \"USA\")\n",
    "\n",
    "display(mcr_usa_bfp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de433d04-0b9f-406e-8a3e-fa70e0d20ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# wkt text as geometry\n",
    "\n",
    "relocated_apt_ds = relocated_apt_ds.withColumn(\"relocated_apt_geom\", F.expr(\"ST_GeomFromWKT(relocated_location)\"))\n",
    "mcr_usa_bfp_dataset = mcr_usa_bfp_dataset.withColumn(\"bfp_geom\", F.expr(\"ST_GeomFromWKT(wkt)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdac3a17-d7b4-4783-9c16-988530efc1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Repartition datasets\n",
    "limited_relocated_apt_ds = relocated_apt_ds.select(\"apt_id\",\"relocated_apt_geom\").repartition(16)\n",
    "mcr_usa_bfp_dataset = mcr_usa_bfp_dataset.select(\"bfp_geom\").repartition(2000, expr(\"ST_GeoHash(bfp_geom, 5)\"))  # Select only necessary columns\n",
    "\n",
    "\n",
    "# Optionally cache the smaller dataset if it's reused multiple times\n",
    "limited_relocated_apt_ds.cache()\n",
    "\n",
    "# Use broadcast join if the BFP dataset is small enough\n",
    "relocated_apt_ds_with_bfp_data = limited_relocated_apt_ds.alias(\"relocated_apt\") \\\n",
    "    .join(mcr_usa_bfp_dataset.alias(\"bfp\"), F.expr(\"ST_Intersects(bfp.bfp_geom, relocated_apt.relocated_apt_geom)\"), \"left\") \\\n",
    "    .withColumn(\"apt_inside_bfp\", F.when(F.col(\"bfp.bfp_geom\").isNull(), False).otherwise(True)) \\\n",
    "    .dropDuplicates([\"relocated_apt_geom\"])\n",
    "\n",
    "# relocated_apt_ds_with_bfp_data = (\n",
    "#     relocated_apt_ds.alias(\"relocated_apt\")\n",
    "#     .join(\n",
    "#         F.broadcast(mcr_usa_bfp_dataset.alias(\"bfp\")),  # Broadcast join\n",
    "#         F.expr(\"ST_Intersects(bfp.bfp_geom, relocated_apt.relocated_apt_geom)\"),\n",
    "#         \"left\"\n",
    "#     )\n",
    "#     .withColumn(\"apt_inside_bfp\", F.when(F.col(\"bfp.bfp_geom\").isNull(), False).otherwise(True))\n",
    "#     .dropDuplicates([\"relocated_apt_geom\"])\n",
    "# )\n",
    "\n",
    "# Optionally unpersist if you no longer need the cached dataset\n",
    "limited_relocated_apt_ds.unpersist()\n",
    "\n",
    "# Proceed with further operations on relocated_apt_ds_with_bfp_data\n",
    "\n",
    "display(relocated_apt_ds_with_bfp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48784ae8-566c-4c9e-968a-1d755863fe23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"viz_dataset relocated_apt_ds: {relocated_apt_ds.count()}\")\n",
    "print(f\"viz_dataset relocated_apt_ds_with_bfp_data: {relocated_apt_ds_with_bfp_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d24fcb-ac76-4d6c-a18f-07b76a7c6d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Repartition datasets and select necessary columns\n",
    "limited_relocated_apt_ds = relocated_apt_ds.select(\"apt_id\", \"relocated_apt_geom\").repartition(16)\n",
    "mcr_usa_bfp_dataset = mcr_usa_bfp_dataset.select(\"bfp_geom\").repartition(16) \n",
    "\n",
    "# Step 2: Perform spatial join to find intersections\n",
    "relocated_apt_ds_with_bfp_data = limited_relocated_apt_ds.alias(\"relocated_apt\") .join(mcr_usa_bfp_dataset.alias(\"bfp\"), F.expr(\"ST_Intersects(relocated_apt.relocated_apt_geom, bfp.bfp_geom)\"),\"left\").withColumn(\"apt_inside_bfp\", F.when(F.col(\"bfp.bfp_geom\").isNull(), False).otherwise(True))\n",
    "\n",
    "# Step 3: Display the results\n",
    "display(relocated_apt_ds_with_bfp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "986da94a-9d2a-45bf-b00f-b1bb6a8ebd68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Intersection with geo panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73edde26-2903-4bb8-8cd3-68b7169e028b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "# Assuming relocated_apt_ds and mcr_usa_bfp_dataset are your DataFrames already defined\n",
    "\n",
    "# Step 1: Repartition datasets and select necessary columns\n",
    "limited_relocated_apt_ds = relocated_apt_ds.select(\"apt_id\", \"relocated_apt_geom\").repartition(16)\n",
    "mcr_usa_bfp_dataset = mcr_usa_bfp_dataset.select(\"bfp_geom\").repartition(2000, expr(\"ST_GeoHash(bfp_geom, 5)\")) \n",
    "#.repartition(128)\n",
    "\n",
    "# Optionally cache the smaller dataset if it's reused multiple times\n",
    "limited_relocated_apt_ds.cache()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "limited_relocated_apt_pd = limited_relocated_apt_ds.toPandas()\n",
    "mcr_usa_bfp_pd = mcr_usa_bfp_dataset.toPandas()\n",
    "\n",
    "# Step 2: Create GeoPandas GeoDataFrames\n",
    "limited_relocated_apt_gdf = gpd.GeoDataFrame(\n",
    "    limited_relocated_apt_pd, \n",
    "    geometry=limited_relocated_apt_pd['relocated_apt_geom'].apply(loads), \n",
    "    crs=\"EPSG:4326\" \n",
    ")\n",
    "\n",
    "mcr_usa_bfp_gdf = gpd.GeoDataFrame(\n",
    "    mcr_usa_bfp_pd, \n",
    "    geometry=mcr_usa_bfp_pd['bfp_geom'].apply(loads), \n",
    "    crs=\"EPSG:4326\"  \n",
    ")\n",
    "\n",
    "# Step 3: Perform spatial join to find intersections\n",
    "intersections = gpd.sjoin(limited_relocated_apt_gdf, mcr_usa_bfp_gdf, op='intersects', how='left')\n",
    "\n",
    "# Optionally filter to keep only matches\n",
    "intersections_filtered = intersections[intersections['index_right'].notnull()]\n",
    "\n",
    "# Step 4: Add a new column indicating whether the apt is inside the BFP\n",
    "intersections_filtered['apt_inside_bfp'] = intersections_filtered['index_right'].notnull()\n",
    "\n",
    "# Step 5: Optionally, save the results to a new GeoJSON or Shapefile\n",
    "# intersections_filtered.to_file(\"intersections_output.geojson\", driver='GeoJSON')\n",
    "\n",
    "# Stop the Spark session if you're done\n",
    "# spark.stop()\n",
    "display(intersections_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "10d4e618-365e-4a0a-9a45-668fc7354c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_partitions = 1000\n",
    "relocated_apt_ds = relocated_apt_ds.repartition(num_partitions)\n",
    "mcr_usa_bfp_dataset = mcr_usa_bfp_dataset.limit(1000000).repartition(num_partitions)\n",
    "\n",
    "relocated_apt_ds_with_bfp_data = (\n",
    "    relocated_apt_ds.alias(\"relocated_apt\")\n",
    "    .join(mcr_usa_bfp_dataset.alias(\"bfp\").hint(\"shuffle\"), \n",
    "          expr(\"ST_Intersects(ST_GeomFromText(bfp.wkt), ST_GeomFromText(relocated_apt.relocated_location))\"), \n",
    "          \"left\")\n",
    "    .withColumn(\"apt_inside_bfp\", when(col(\"bfp.wkt\").isNull(), False).otherwise(True))\n",
    ")\n",
    "\n",
    "display(relocated_apt_ds_with_bfp_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Find-Roof-Top-Metrics-On-Relocated-APT-CSV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
